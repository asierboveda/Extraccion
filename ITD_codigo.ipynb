{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c892466",
   "metadata": {},
   "source": [
    "### ITD\n",
    "En este dataset tenemos fotos tomadas con 10 cÃ¡maras diferentes a diferentes telas. De cada cÃ¡mara tenemos una parte de las fotos para train, en las que solamente hay fotos de telas en buen estado. Y otra parte para test, en las que hay fotos de telas en buen estado y daÃ±adas.\n",
    "El objetivo es entrenar el modelo solo con las fotos de train (tela en buen estado) y poder predecir si una foto de test es una tela en buen estado ($y_{test}=0$) o daÃ±ada ($y_{test}=1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc72b7a",
   "metadata": {},
   "source": [
    "## Leer datos todos juntos\n",
    "\n",
    "Leemos mezclando test y train, y luego particioanremos todos los datos en train, val y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecc6f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Escaneando imÃ¡genes en: c:\\Users\\ander\\OneDrive - UPNA\\4Âº\\ExtracciÃ³n del conocimiento\\TrabajoGrupo\\ITD ...\n",
      "âœ… Carga completa. 5878 imÃ¡genes procesadas.\n",
      "\n",
      "Forma de X_total: (5878, 64, 64)\n",
      "------------------------------\n",
      "ðŸ”¹ Train shape: (4114, 64, 64) (Listo para Autoencoders/CNN)\n",
      "ðŸ”¹ Val shape:   (882, 64, 64)\n",
      "ðŸ”¹ Test shape:  (882, 64, 64)\n",
      "------------------------------\n",
      "Train anomalies: 675\n",
      "Val anomalies:   145\n",
      "Test anomalies:  145\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def cargar_dataset_imagenes(root_path, image_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    Lee las imÃ¡genes y las devuelve en formato (N, Alto, Ancho).\n",
    "    NO aplana las imÃ¡genes. Mantiene la estructura espacial.\n",
    "    \"\"\"\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    root = Path(root_path)\n",
    "    print(f\"ðŸ“‚ Escaneando imÃ¡genes en: {root.absolute()} ...\")\n",
    "    \n",
    "    if not root.exists():\n",
    "        print(\"âŒ Error: La ruta no existe.\")\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    contador = 0\n",
    "    \n",
    "    for img_path in root.rglob('*.*'):\n",
    "        if img_path.suffix.lower() not in ['.png', '.jpg', '.jpeg', '.bmp']:\n",
    "            continue\n",
    "            \n",
    "        # --- 1. ETIQUETADO ---\n",
    "        # 1 = Defecto (Anomaly), 0 = Bien (Good)\n",
    "        label = 1 if 'anomaly' in str(img_path).lower() else 0\n",
    "            \n",
    "        # --- 2. LECTURA (Escala de Grises) ---\n",
    "        # Leemos en blanco y negro para simplificar (1 canal). \n",
    "        # Si quisieras color, quita el flag IMREAD_GRAYSCALE.\n",
    "        img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if img is None: continue\n",
    "        \n",
    "        # --- 3. RESIZE (Obligatorio) ---\n",
    "        # Todas deben medir lo mismo para entrar en el array numpy\n",
    "        img = cv2.resize(img, image_size)\n",
    "        \n",
    "        # --- 4. NORMALIZACIÃ“N ---\n",
    "        # Pasamos de 0-255 (enteros) a 0.0-1.0 (float).\n",
    "        # Esto es vital tanto para ML clÃ¡sico como para Deep Learning.\n",
    "        img_norm = img.astype('float32') / 255.0\n",
    "        \n",
    "        X_list.append(img_norm)\n",
    "        y_list.append(label)\n",
    "        contador += 1\n",
    "\n",
    "    # Convertimos la lista a un Array Numpy 3D: (N_fotos, Alto, Ancho)\n",
    "    X_array = np.array(X_list)\n",
    "    y_array = np.array(y_list)\n",
    "\n",
    "    print(f\"âœ… Carga completa. {contador} imÃ¡genes procesadas.\")\n",
    "    return X_array, y_array\n",
    "\n",
    "# --- EJECUCIÃ“N: CARGAR Y DIVIDIR ---\n",
    "\n",
    "ruta = \"./ITD\" # Tu ruta\n",
    "X_total, y_total = cargar_dataset_imagenes(ruta, image_size=(64, 64))\n",
    "\n",
    "# VerificaciÃ³n de forma\n",
    "# DeberÃ­a salir algo como: (5000, 64, 64) -> 5000 fotos de 64x64\n",
    "print(f\"\\nForma de X_total: {X_total.shape}\") \n",
    "\n",
    "# --- PARTICIÃ“N (Train / Val / Test) ---\n",
    "# 1. Separamos Train (70%) del resto (30%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_total, y_total, test_size=0.3, random_state=42, stratify=y_total\n",
    ")\n",
    "\n",
    "# 2. Separamos el resto en Val (15%) y Test (15%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"ðŸ”¹ Train shape: {X_train.shape} (Listo para Autoencoders/CNN)\")\n",
    "print(f\"ðŸ”¹ Val shape:   {X_val.shape}\")\n",
    "print(f\"ðŸ”¹ Test shape:  {X_test.shape}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# mostramos numero de anomalias en cada conjunto\n",
    "print(f\"Train anomalies: {np.sum(y_train)}\")\n",
    "print(f\"Val anomalies:   {np.sum(y_val)}\")\n",
    "print(f\"Test anomalies:  {np.sum(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86362f16",
   "metadata": {},
   "source": [
    "Ahora la forma de leerlos de train (solo good) y test (good y anomaly) por separado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "581d0dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Escaneando estructura original en: c:\\Users\\Mikel\\Desktop\\EXTRACCIÃ“N DEL CONOCIMIENTO\\Trabajo\\Extraccion\\ITD ...\n",
      "âœ… Carga finalizada.\n",
      "   Train: 4391 imÃ¡genes.\n",
      "   Test:  1141 imÃ¡genes.\n",
      "\n",
      "--- DIMENSIONES (RAW) ---\n",
      "X_train: (4391, 64, 64)\n",
      "y_train: (4391,)\n",
      "X_test:  (1141, 64, 64)\n",
      "y_test:  (1141,)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def cargar_datos_separados_raw(root_path, image_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    Lee las carpetas originales 'train' y 'test' y devuelve los datos separados.\n",
    "    Formato: RAW (PÃ­xeles 2D, normalizados 0-1).\n",
    "    \"\"\"\n",
    "    # Listas para Entrenamiento\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    # Listas para Test\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    root = Path(root_path)\n",
    "    print(f\"ðŸ“‚ Escaneando estructura original en: {root.absolute()} ...\")\n",
    "    \n",
    "    if not root.exists():\n",
    "        print(\"âŒ Error: La ruta no existe.\")\n",
    "        return np.array([]), np.array([]), np.array([]), np.array([])\n",
    "\n",
    "    contador_train = 0\n",
    "    contador_test = 0\n",
    "    \n",
    "    for img_path in root.rglob('*.*'):\n",
    "        if img_path.suffix.lower() not in ['.png', '.jpg', '.jpeg', '.bmp']:\n",
    "            continue\n",
    "        \n",
    "        path_str = str(img_path).lower()\n",
    "        \n",
    "        # --- 1. IDENTIFICAR SPLIT (Â¿Es Train o Test?) ---\n",
    "        if 'train' in path_str:\n",
    "            is_train = True\n",
    "        elif 'test' in path_str:\n",
    "            is_train = False\n",
    "        else:\n",
    "            continue # Si no estÃ¡ en ninguna carpeta train/test, la ignoramos\n",
    "            \n",
    "        # --- 2. IDENTIFICAR ETIQUETA (Â¿Good o Anomaly?) ---\n",
    "        # 1 = Defecto, 0 = Bien\n",
    "        label = 1 if 'anomaly' in path_str else 0\n",
    "        \n",
    "        # --- 3. LEER RAW (Escala de Grises) ---\n",
    "        img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if img is None: continue\n",
    "        \n",
    "        # --- 4. RESIZE & NORMALIZACIÃ“N ---\n",
    "        img = cv2.resize(img, image_size)\n",
    "        img_norm = img.astype('float32') / 255.0\n",
    "        \n",
    "        # --- 5. GUARDAR DONDE TOQUE ---\n",
    "        if is_train:\n",
    "            X_train.append(img_norm)\n",
    "            y_train.append(label)\n",
    "            contador_train += 1\n",
    "        else:\n",
    "            X_test.append(img_norm)\n",
    "            y_test.append(label)\n",
    "            contador_test += 1\n",
    "\n",
    "    # Convertir todo a Numpy Arrays\n",
    "    X_train_arr = np.array(X_train)\n",
    "    y_train_arr = np.array(y_train)\n",
    "    X_test_arr = np.array(X_test)\n",
    "    y_test_arr = np.array(y_test)\n",
    "\n",
    "    print(f\"âœ… Carga finalizada.\")\n",
    "    print(f\"   Train: {contador_train} imÃ¡genes.\")\n",
    "    print(f\"   Test:  {contador_test} imÃ¡genes.\")\n",
    "    \n",
    "    return X_train_arr, y_train_arr, X_test_arr, y_test_arr\n",
    "\n",
    "# --- USO ---\n",
    "ruta = \"./ITD\" # AsegÃºrate que es la ruta correcta\n",
    "X_train, y_train, X_test, y_test = cargar_datos_separados_raw(ruta, image_size=(64, 64))\n",
    "\n",
    "print(\"\\n--- DIMENSIONES (RAW) ---\")\n",
    "# DeberÃ­a salir (N, 64, 64) -> Datos 2D listos para lo que quieras\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"X_test:  {X_test.shape}\")\n",
    "print(f\"y_test:  {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cea8bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma aplanada para ML ClÃ¡sico: (4391, 4096)\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo rÃ¡pido para Isolation Forest\n",
    "# -1 le dice a numpy: \"calcula tÃº esta dimensiÃ³n\" (que serÃ¡ 64*64 = 4096)\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1) \n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "print(f\"Forma aplanada para ML ClÃ¡sico: {X_train_flat.shape}\") \n",
    "# Resultado: (N, 4096)\n",
    "\n",
    "# clf.fit(X_train_flat) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0379c9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo mental para futuro (PyTorch)\n",
    "# Solo tendrÃ­as que aÃ±adir una dimensiÃ³n de canal: (N, 1, 64, 64)\n",
    "# tensor_img = torch.from_numpy(X_train).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba82246",
   "metadata": {},
   "source": [
    "Entrenar y predecir con OneClass + SVM. \n",
    "El One-Class SVM no es un clasificador supervisado tradicional (como un SVM binario con clases 0 y 1).\n",
    "Es un mÃ©todo de detecciÃ³n de anomalÃ­as o novelty detection, cuyo objetivo es aprender la forma del conjunto de datos â€œnormalesâ€ y luego identificar puntos que no se ajustan a esa distribuciÃ³n. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63958d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando One-Class SVM con 4391 imÃ¡genes buenas...\n",
      "âœ… Entrenamiento completado.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "X_train_good = X_train_flat[y_train == 0]\n",
    "\n",
    "print(f\"Entrenando One-Class SVM con {X_train_good.shape[0]} imÃ¡genes buenas...\")\n",
    "\n",
    "ocsvm = OneClassSVM(\n",
    "    kernel=\"rbf\",     # kernel radial: suele funcionar bien para este tipo de datos\n",
    "    nu=0.01,          # proporciÃ³n esperada de anomalÃ­as\n",
    "    gamma=\"scale\"     # parÃ¡metro del kernel RBF (puede ajustarse)\n",
    ")\n",
    "\n",
    "ocsvm.fit(X_train_good)\n",
    "print(\"âœ… Entrenamiento completado.\")\n",
    "\n",
    "\n",
    "y_pred_test = ocsvm.predict(X_test_flat)\n",
    "\n",
    "# Convertimos a 0 = normal, 1 = anÃ³malo\n",
    "y_pred_test = np.where(y_pred_test == -1, 1, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e08c7297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š MATRIZ DE CONFUSIÃ“N (Test):\n",
      "[[500  12]\n",
      " [431 198]]\n",
      "\n",
      "ðŸ“ˆ MÃ‰TRICAS (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.537     0.977     0.693       512\n",
      "           1      0.943     0.315     0.472       629\n",
      "\n",
      "    accuracy                          0.612      1141\n",
      "   macro avg      0.740     0.646     0.582      1141\n",
      "weighted avg      0.761     0.612     0.571      1141\n",
      "\n",
      "ROC-AUC (Test): 0.646\n",
      "\n",
      "ðŸ” AnomalÃ­as detectadas en test: 210 / 1141\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 4ï¸âƒ£ EvaluaciÃ³n\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nðŸ“Š MATRIZ DE CONFUSIÃ“N (Test):\")\n",
    "print(confusion_matrix(y_test, y_pred_test))\n",
    "\n",
    "print(\"\\nðŸ“ˆ MÃ‰TRICAS (Test):\")\n",
    "print(classification_report(y_test, y_pred_test, digits=3))\n",
    "\n",
    "try:\n",
    "    auc = roc_auc_score(y_test, y_pred_test)\n",
    "    print(f\"ROC-AUC (Test): {auc:.3f}\")\n",
    "except Exception as e:\n",
    "    print(\"No se pudo calcular AUC:\", e)\n",
    "\n",
    "detected_anomalies = np.sum(y_pred_test)\n",
    "print(f\"\\nðŸ” AnomalÃ­as detectadas en test: {detected_anomalies} / {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b1b722",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e633cf46",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, models\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m roc_auc_score, classification_report\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1ï¸âƒ£ Preprocesamiento\n",
    "# ------------------------------------------------------------\n",
    "# Si tus datos son (N, 64, 64) -> agrega canal (N, 64, 64, 1)\n",
    "def preparar_datos(X):\n",
    "    X = np.expand_dims(X, axis=-1)\n",
    "    return X.astype('float32')\n",
    "\n",
    "X_train_prep = preparar_datos(X_train)\n",
    "X_test_prep  = preparar_datos(X_test)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2ï¸âƒ£ DefiniciÃ³n del modelo Autoencoder (CNN)\n",
    "# ------------------------------------------------------------\n",
    "def build_autoencoder(input_shape=(64, 64, 1)):\n",
    "    encoder = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, (3,3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "        layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2,2))\n",
    "    ])\n",
    "    \n",
    "    decoder = models.Sequential([\n",
    "        layers.Conv2DTranspose(64, (3,3), strides=2, activation='relu', padding='same'),\n",
    "        layers.Conv2DTranspose(32, (3,3), strides=2, activation='relu', padding='same'),\n",
    "        layers.Conv2D(1, (3,3), activation='sigmoid', padding='same')\n",
    "    ])\n",
    "    \n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    encoded = encoder(inp)\n",
    "    decoded = decoder(encoded)\n",
    "    \n",
    "    autoencoder = models.Model(inp, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return autoencoder\n",
    "\n",
    "autoencoder = build_autoencoder(input_shape=(64, 64, 1))\n",
    "autoencoder.summary()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3ï¸âƒ£ Entrenamiento\n",
    "# ------------------------------------------------------------\n",
    "# Si usas el esquema One-Class:\n",
    "# solo imÃ¡genes buenas (sin anomalÃ­as)\n",
    "X_train_good = X_train_prep[y_train == 0] if 'y_train' in locals() else X_train_prep\n",
    "\n",
    "history = autoencoder.fit(\n",
    "    X_train_good, X_train_good,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_split=0.1,  # o usa X_val si tienes el split separado\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4ï¸âƒ£ EvaluaciÃ³n (detecciÃ³n de anomalÃ­as)\n",
    "# ------------------------------------------------------------\n",
    "# 1. Reconstruimos las imÃ¡genes del test\n",
    "X_test_pred = autoencoder.predict(X_test_prep)\n",
    "\n",
    "# 2. Calculamos el error de reconstrucciÃ³n\n",
    "recon_error = np.mean(np.square(X_test_prep - X_test_pred), axis=(1,2,3))\n",
    "\n",
    "# 3. Fijamos un umbral (por ejemplo, percentil 95 de los errores de train)\n",
    "threshold = np.percentile(\n",
    "    np.mean(np.square(X_train_good - autoencoder.predict(X_train_good)), axis=(1,2,3)),\n",
    "    95\n",
    ")\n",
    "\n",
    "# 4. Clasificamos segÃºn el umbral\n",
    "y_pred = (recon_error > threshold).astype(int)\n",
    "\n",
    "print(\"\\nðŸ“Š RESULTADOS (Test):\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, recon_error):.3f}\")\n",
    "print(f\"Umbral elegido: {threshold:.6f}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
